{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3c21ee-99b6-4885-9523-8ca365a3c0e1",
   "metadata": {},
   "source": [
    "# for unzip the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857cdc07-556f-4dc7-bb27-4dcab26b0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# zip_path = \"archive.zip\"      # Path to the zip file\n",
    "# extract_to = \"data\"   # Folder where files will be extracted\n",
    "\n",
    "# # Create folder if it does not exist\n",
    "# os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_to)\n",
    "\n",
    "# print(\"Unzipping completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00308e1-eb3b-4040-a55f-5f865b3cd3e4",
   "metadata": {},
   "source": [
    "# simple preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ac8df0-f083-4060-bc03-3d9fbee83861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282e4cb9-451b-44d0-b55c-4c2f3823a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val = pd.read_csv(\"data//samsum-validation.csv\")\n",
    "# df_train = pd.read_csv(\"data//samsum-train.csv\")\n",
    "# df_test = pd.read_csv(\"data//samsum-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b493956-4587-4d8b-a9fa-ca6112ea5d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>Amanda: I baked  cookies. Do you want some?\\r\\...</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>Olivia: Who are you voting for in this electio...</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...</td>\n",
       "      <td>Kim may try the pomodoro technique recommended...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13730747</td>\n",
       "      <td>Edward: Rachel, I think I'm in ove with Bella....</td>\n",
       "      <td>Edward thinks he is in love with Bella. Rachel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           dialogue  \\\n",
       "0  13818513  Amanda: I baked  cookies. Do you want some?\\r\\...   \n",
       "1  13728867  Olivia: Who are you voting for in this electio...   \n",
       "2  13681000  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...   \n",
       "3  13730747  Edward: Rachel, I think I'm in ove with Bella....   \n",
       "\n",
       "                                             summary  \n",
       "0  Amanda baked cookies and will bring Jerry some...  \n",
       "1  Olivia and Olivier are voting for liberals in ...  \n",
       "2  Kim may try the pomodoro technique recommended...  \n",
       "3  Edward thinks he is in love with Bella. Rachel...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe1bc0e-45b6-4a5e-9e6c-7a4a41a9925d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13862856</td>\n",
       "      <td>Hannah: Hey, do you have Betty's number?\\nAman...</td>\n",
       "      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13729565</td>\n",
       "      <td>Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...</td>\n",
       "      <td>Eric and Rob are going to watch a stand-up on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13680171</td>\n",
       "      <td>Lenny: Babe, can you help me with something?\\r...</td>\n",
       "      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           dialogue  \\\n",
       "0  13862856  Hannah: Hey, do you have Betty's number?\\nAman...   \n",
       "1  13729565  Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...   \n",
       "2  13680171  Lenny: Babe, can you help me with something?\\r...   \n",
       "\n",
       "                                             summary  \n",
       "0  Hannah needs Betty's number but Amanda doesn't...  \n",
       "1  Eric and Rob are going to watch a stand-up on ...  \n",
       "2  Lenny can't decide which trousers to buy. Bob ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d89ad983-dd19-4bdc-aa04-4f0dfe6c89b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13817023</td>\n",
       "      <td>A: Hi Tom, are you busy tomorrow’s afternoon?\\...</td>\n",
       "      <td>A will go to the animal shelter tomorrow to ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13716628</td>\n",
       "      <td>Emma: I’ve just fallen in love with this adven...</td>\n",
       "      <td>Emma and Rob love the advent calendar. Lauren ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13829420</td>\n",
       "      <td>Jackie: Madison is pregnant\\r\\nJackie: but she...</td>\n",
       "      <td>Madison is pregnant but she doesn't want to ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13819648</td>\n",
       "      <td>Marla: &lt;file_photo&gt;\\r\\nMarla: look what I foun...</td>\n",
       "      <td>Marla found a pair of boxers under her bed.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           dialogue  \\\n",
       "0  13817023  A: Hi Tom, are you busy tomorrow’s afternoon?\\...   \n",
       "1  13716628  Emma: I’ve just fallen in love with this adven...   \n",
       "2  13829420  Jackie: Madison is pregnant\\r\\nJackie: but she...   \n",
       "3  13819648  Marla: <file_photo>\\r\\nMarla: look what I foun...   \n",
       "\n",
       "                                             summary  \n",
       "0  A will go to the animal shelter tomorrow to ge...  \n",
       "1  Emma and Rob love the advent calendar. Lauren ...  \n",
       "2  Madison is pregnant but she doesn't want to ta...  \n",
       "3        Marla found a pair of boxers under her bed.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfed2c71-d5e6-481b-b682-b349453f98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return text.strip().replace(\"\\n\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c93fbaa-b06f-4dcc-b173-93ce55b3eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_val, df_test]:\n",
    "    df[\"dialogue\"] = df[\"dialogue\"].apply(clean_text)\n",
    "    df[\"summary\"] = df[\"summary\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "085b695b-b023-4fda-a30c-88d73259eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.rename(columns={\"dialogue\": \"input_text\", \"summary\": \"target_text\"})\n",
    "df_val = df_val.rename(columns={\"dialogue\": \"input_text\", \"summary\": \"target_text\"})\n",
    "df_test = df_test.rename(columns={\"dialogue\": \"input_text\",\"summary\": \"target_text\"})  # test has no summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d773c3d-bfb5-4968-b4d6-5a5bb87a3793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"data/processed_train.csv\", index=False)\n",
    "df_val.to_csv(\"data/processed_val.csv\", index=False)\n",
    "df_test.to_csv(\"data/processed_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d6cce-d2ee-46b3-8344-87e1fb058b74",
   "metadata": {},
   "source": [
    "# Train start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227c7e7a-bcc6-4c86-83a1-c0e68dc8bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"data/processed_train.csv\")\n",
    "val_df   = pd.read_csv(\"data/processed_val.csv\")\n",
    "test_df  = pd.read_csv(\"data/processed_test.csv\")\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df[\"input_text\"] = df[\"input_text\"].astype(str)\n",
    "    df[\"target_text\"] = df[\"target_text\"].astype(str)\n",
    "\n",
    "    # Replace 'nan', 'None', 'NaN', float nan with empty string\n",
    "    df[\"input_text\"] = df[\"input_text\"].replace(\"nan\", \"\").replace(\"None\", \"\")\n",
    "    df[\"target_text\"] = df[\"target_text\"].replace(\"nan\", \"\").replace(\"None\", \"\")\n",
    "\n",
    "    df[\"input_text\"] = df[\"input_text\"].fillna(\"\")\n",
    "    df[\"target_text\"] = df[\"target_text\"].fillna(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c022b80f-4d09-4e8f-b34e-d06a5cfb9ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class SummDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_input=256, max_output=64):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input = max_input\n",
    "        self.max_output = max_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        input_text = \"summarize: \" + str(row[\"input_text\"])\n",
    "        target_text = str(row[\"target_text\"])\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_input\n",
    "        )\n",
    "\n",
    "        labels = self.tokenizer(\n",
    "            target_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_output\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # Replace pad tokens with -100\n",
    "        labels = [l if l != self.tokenizer.pad_token_id else -100 for l in labels]\n",
    "\n",
    "        inputs[\"labels\"] = labels\n",
    "\n",
    "        return {k: torch.tensor(v) for k, v in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f63f6-00cf-4633-9ee7-c520057b96ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f1313e-7a63-4e2e-9c48-097891afa3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/cghosh/tf_gpu/tfgpu1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5-base loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"T5-base loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c017a6-7421-4b59-bd6f-84b040ff1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y transformers peft\n",
    "# !pip install transformers==4.44.2 peft==0.12.0 accelerate\n",
    "# !pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a626c2-2219-4e19-98ad-8642b4caefec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = SummDataset(train_df, tokenizer)\n",
    "val_dataset = SummDataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741c2a30-69f8-46a2-9252-01127d576d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from tqdm.auto import tqdm\n",
    "# # from torch.optim import AdamW\n",
    "# # from rouge_score import rouge_scorer\n",
    "\n",
    "# # optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "# # scaler = torch.cuda.amp.GradScaler()\n",
    "# # scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# # accum_steps = 4\n",
    "# # epochs = 5\n",
    "\n",
    "# # for epoch in range(epochs):\n",
    "# #     model.train()\n",
    "# #     progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "# #     total_loss = 0\n",
    "\n",
    "# #     for i, batch in enumerate(progress):\n",
    "# #         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "# #         with torch.amp.autocast(\"cuda\"):\n",
    "# #             outputs = model(**batch)\n",
    "# #             loss = outputs.loss / accum_steps\n",
    "\n",
    "# #         scaler.scale(loss).backward()\n",
    "\n",
    "# #         if (i + 1) % accum_steps == 0:\n",
    "# #             scaler.step(optimizer)\n",
    "# #             scaler.update()\n",
    "# #             optimizer.zero_grad()\n",
    "\n",
    "# #         total_loss += loss.item()\n",
    "\n",
    "# #         # -------------------------\n",
    "# #         # ROUGE every 200 steps\n",
    "# #         # -------------------------\n",
    "# #         if i % 200 == 0:\n",
    "# #             model.eval()\n",
    "# #             with torch.no_grad():\n",
    "# #                 summary_ids = model.generate(\n",
    "# #                     batch[\"input_ids\"][0].unsqueeze(0),\n",
    "# #                     attention_mask=batch[\"attention_mask\"][0].unsqueeze(0),\n",
    "# #                     max_new_tokens=64\n",
    "# #                 )\n",
    "\n",
    "# #                 pred = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "# #                 true = tokenizer.decode(\n",
    "# #                     [x for x in batch[\"labels\"][0].tolist() if x != -100],\n",
    "# #                     skip_special_tokens=True\n",
    "# #                 )\n",
    "\n",
    "# #                 rouge = scorer.score(true, pred)[\"rougeL\"].fmeasure\n",
    "\n",
    "# #             model.train()\n",
    "# #             progress.set_postfix({\n",
    "# #                 \"loss\": f\"{loss.item():.4f}\",\n",
    "# #                 \"rougeL\": f\"{rouge:.4f}\"\n",
    "# #             })\n",
    "\n",
    "# #     print(f\"Epoch {epoch+1} | Total Loss = {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "# from torch.optim import AdamW\n",
    "# from rouge_score import rouge_scorer\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "# scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# accum_steps = 4\n",
    "# epochs = 15\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "#     total_loss = 0\n",
    "\n",
    "#     # -------------------------\n",
    "#     # TRAINING LOOP\n",
    "#     # -------------------------\n",
    "#     for i, batch in enumerate(progress):\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "#         with torch.amp.autocast(\"cuda\"):\n",
    "#             outputs = model(**batch)\n",
    "#             loss = outputs.loss / accum_steps\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "\n",
    "#         if (i + 1) % accum_steps == 0:\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Small training ROUGE sample print every 200 steps\n",
    "#         if i % 200 == 0:\n",
    "#             model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 summary_ids = model.generate(\n",
    "#                     batch[\"input_ids\"][0].unsqueeze(0),\n",
    "#                     attention_mask=batch[\"attention_mask\"][0].unsqueeze(0),\n",
    "#                     max_new_tokens=64\n",
    "#                 )\n",
    "#                 pred = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "#                 true = tokenizer.decode(\n",
    "#                     [x for x in batch[\"labels\"][0].tolist() if x != -100],\n",
    "#                     skip_special_tokens=True\n",
    "#                 )\n",
    "#                 rouge = scorer.score(true, pred)[\"rougeL\"].fmeasure\n",
    "\n",
    "#             model.train()\n",
    "#             progress.set_postfix({\n",
    "#                 \"loss\": f\"{loss.item():.4f}\",\n",
    "#                 \"rougeL\": f\"{rouge:.4f}\"\n",
    "#             })\n",
    "\n",
    "#     print(f\"\\nEpoch {epoch+1} | Training Loss = {total_loss:.4f}\")\n",
    "\n",
    "#     # -------------------------\n",
    "#     # VALIDATION LOOP\n",
    "#     # -------------------------\n",
    "#     model.eval()\n",
    "#     val_rouge_scores = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attn_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "#             labels = batch[\"labels\"].tolist()\n",
    "\n",
    "#             summary_ids = model.generate(input_ids, attention_mask=attn_mask, max_new_tokens=64)\n",
    "\n",
    "#             for pred_tokens, true_tokens in zip(summary_ids, labels):\n",
    "#                 pred = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "#                 true = tokenizer.decode([x for x in true_tokens if x != -100], skip_special_tokens=True)\n",
    "\n",
    "#                 score = scorer.score(true, pred)[\"rougeL\"].fmeasure\n",
    "#                 val_rouge_scores.append(score)\n",
    "\n",
    "#     avg_val_rouge = sum(val_rouge_scores) / len(val_rouge_scores)\n",
    "#     print(f\"Epoch {epoch+1} | Validation ROUGE-L = {avg_val_rouge:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18c6d58b-ba4a-4e10-bb53-d0d36bfcef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "\n",
      "Trying checkpoint: epoch_9\n",
      "Failed to load checkpoint epoch_9: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "\n",
      "Trying checkpoint: epoch_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_737/424661384.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if device == \"cuda\" else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state_dict from epoch_8\n",
      "Loaded optimizer\n",
      "Loaded scaler\n",
      "Resuming from epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ada098b11f64c92874fbe2c7eeef96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/3683 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Training Loss = nan\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbe8ca9e87748dfa051a9f22ecd0f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Validation ROUGE-L = 0.3761\n",
      "Checkpoint saved: checkpoints/epoch_9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# -----------------------------\n",
    "# DEVICE FIX\n",
    "# -----------------------------\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# OPTIMIZER, SCALER, ROUGE\n",
    "# -----------------------------\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "scaler = torch.cuda.amp.GradScaler() if device == \"cuda\" else None\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "accum_steps = 4\n",
    "epochs = 10\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "# Create checkpoint folder\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# LOAD CHECKPOINT IF AVAILABLE\n",
    "# ==========================================================\n",
    "start_epoch = 0\n",
    "\n",
    "def load_latest_checkpoint():\n",
    "    global start_epoch\n",
    "\n",
    "    folders = [f for f in os.listdir(CHECKPOINT_DIR) if f.startswith(\"epoch_\")]\n",
    "    if not folders:\n",
    "        print(\"No checkpoint found. Starting fresh.\")\n",
    "        return\n",
    "\n",
    "    # Sort descending → try newest first\n",
    "    folders = sorted(folders, key=lambda x: int(x.split(\"_\")[1]), reverse=True)\n",
    "\n",
    "    for folder in folders:\n",
    "        path = os.path.join(CHECKPOINT_DIR, folder)\n",
    "        model_file = f\"{path}/model.pt\"\n",
    "\n",
    "        print(f\"\\nTrying checkpoint: {folder}\")\n",
    "\n",
    "        try:\n",
    "            # Try state_dict load\n",
    "            state = torch.load(model_file, map_location=device)\n",
    "            model.load_state_dict(state)\n",
    "            print(f\"Loaded state_dict from {folder}\")\n",
    "\n",
    "            # Load optimizer\n",
    "            optimizer_path = f\"{path}/optimizer.pt\"\n",
    "            if os.path.exists(optimizer_path):\n",
    "                optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
    "                print(\"Loaded optimizer\")\n",
    "\n",
    "            # Load scaler (if CUDA)\n",
    "            if scaler:\n",
    "                scaler_path = f\"{path}/scaler.pt\"\n",
    "                if os.path.exists(scaler_path):\n",
    "                    scaler.load_state_dict(torch.load(scaler_path, map_location=device))\n",
    "                    print(\"Loaded scaler\")\n",
    "\n",
    "            # Load training state\n",
    "            state_file = f\"{path}/training_state.pt\"\n",
    "            if os.path.exists(state_file):\n",
    "                ts = torch.load(state_file)\n",
    "                start_epoch = ts[\"epoch\"] + 1\n",
    "                print(f\"Resuming from epoch {start_epoch}\")\n",
    "            else:\n",
    "                print(\"No training_state.pt, starting next epoch\")\n",
    "\n",
    "            return  # SUCCESS → STOP LOOP\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load checkpoint {folder}: {e}\")\n",
    "\n",
    "    print(\"No valid checkpoints found. Starting fresh.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "load_latest_checkpoint()\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# TRAINING LOOP WITH CHECKPOINT SAVE\n",
    "# ==========================================================\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    total_loss = 0\n",
    "\n",
    "    # -------------------------\n",
    "    # TRAIN LOOP\n",
    "    # -------------------------\n",
    "    for i, batch in enumerate(progress):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Only use autocast for CUDA\n",
    "        if device == \"cuda\":\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss / accum_steps\n",
    "        else:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / accum_steps\n",
    "\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        if (i + 1) % accum_steps == 0:\n",
    "            if scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Show ROUGE every 200 steps\n",
    "        if i % 200 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                summary_ids = model.generate(\n",
    "                    batch[\"input_ids\"][0].unsqueeze(0),\n",
    "                    attention_mask=batch[\"attention_mask\"][0].unsqueeze(0),\n",
    "                    max_new_tokens=64\n",
    "                )\n",
    "                pred = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "                true = tokenizer.decode(\n",
    "                    [x for x in batch[\"labels\"][0].tolist() if x != -100],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                rouge = scorer.score(true, pred)[\"rougeL\"].fmeasure\n",
    "\n",
    "            model.train()\n",
    "            progress.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"rougeL\": f\"{rouge:.4f}\"\n",
    "            })\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} | Training Loss = {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # -------------------------\n",
    "    # VALIDATION LOOP\n",
    "    # -------------------------\n",
    "    model.eval()\n",
    "    val_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].tolist()\n",
    "\n",
    "            summary_ids = model.generate(input_ids, attention_mask=attn)\n",
    "\n",
    "            for pred_ids, true_ids in zip(summary_ids, labels):\n",
    "                pred = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "                true = tokenizer.decode([x for x in true_ids if x != -100], skip_special_tokens=True)\n",
    "                val_scores.append(scorer.score(true, pred)[\"rougeL\"].fmeasure)\n",
    "\n",
    "    avg_rouge = sum(val_scores) / len(val_scores)\n",
    "    print(f\"Epoch {epoch+1} | Validation ROUGE-L = {avg_rouge:.4f}\")\n",
    "\n",
    "\n",
    "    # ======================================================\n",
    "    # SAVE CHECKPOINT SAFELY\n",
    "    # ======================================================\n",
    "    save_path = f\"{CHECKPOINT_DIR}/epoch_{epoch}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{save_path}/model.pt\")\n",
    "    torch.save(optimizer.state_dict(), f\"{save_path}/optimizer.pt\")\n",
    "\n",
    "    if scaler:\n",
    "        torch.save(scaler.state_dict(), f\"{save_path}/scaler.pt\")\n",
    "\n",
    "    torch.save({\"epoch\": epoch}, f\"{save_path}/training_state.pt\")\n",
    "\n",
    "    print(f\"Checkpoint saved: {save_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927e30d-0cd2-4cd0-a0d5-9d36b1c5abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_summary(text):\n",
    "#     inputs = tokenizer([text], return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "#     output = model.generate(\n",
    "#         **inputs,\n",
    "#         max_new_tokens=128,\n",
    "#         num_beams=4,\n",
    "#         length_penalty=2.0,\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "\n",
    "#     return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80ee49-6f75-4f6c-af53-a8ee0723c6e4",
   "metadata": {},
   "source": [
    "# for model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be9f8d8-8b96-4c2f-84ef-2f92dda605fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save sucessfully\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"t5_summarizer/\")\n",
    "tokenizer.save_pretrained(\"t5_summarizer/\")\n",
    "print(\"Model save sucessfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39a2cf-9631-483f-b476-d06a2341676c",
   "metadata": {},
   "source": [
    "# use save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e6a88f9-a2c2-4fca-97c8-ebde51d37ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from: t5_summarizer/\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load your saved model + tokenizer\n",
    "# --------------------------------------------------\n",
    "\n",
    "model_dir = \"t5_summarizer/\"   # PATH of your saved model folder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully from:\", model_dir)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Summarization Function (Improved)\n",
    "# --------------------------------------------------\n",
    "def summarize_text(text):\n",
    "    \"\"\"\n",
    "    Smart summarizer:\n",
    "    - auto-adjusts summary length based on input size\n",
    "    - prevents premature stopping\n",
    "    - avoids repetition\n",
    "    - ensures output is complete\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 1. Measure input length\n",
    "    # ----------------------------------------\n",
    "    input_len = len(tokenizer.encode(text))\n",
    "\n",
    "    # Auto length selection (optimized)\n",
    "    if input_len < 80:\n",
    "        out_len = 30\n",
    "    elif input_len < 150:\n",
    "        out_len = 40\n",
    "    elif input_len < 250:\n",
    "        out_len = 70\n",
    "    elif input_len < 350:\n",
    "        out_len = 110\n",
    "    else:\n",
    "        out_len = 130\n",
    "  # Cap to avoid GPU overload\n",
    "\n",
    "    # Minimum length (very important)\n",
    "    min_len = max(30, out_len // 3)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2. Tokenize Input\n",
    "    # ----------------------------------------\n",
    "    inputs = tokenizer(\n",
    "        \"summarize: \" + text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512   # T5-base limit\n",
    "    ).to(device)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 3. Generate Summary with Safe Settings\n",
    "    # ----------------------------------------\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=out_len,\n",
    "            min_length=min_len,\n",
    "            num_beams=4,                 # 4 beams is more stable than 6\n",
    "            no_repeat_ngram_size=2,      # safer, less cutting\n",
    "            repetition_penalty=1.3,      # balanced; avoids early cutoff\n",
    "            length_penalty=0.8,          # allows longer output\n",
    "            early_stopping=False,\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 4. Decode Output\n",
    "    # ----------------------------------------\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce130a0-eb22-4ad7-a46d-fd8a15ace3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== SUMMARY ==========\n",
      "\n",
      "User's code shows a null pointer exception on line 42. Agent advises User to check the object or initialize it before use\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"\"\"\n",
    "   User: My code keeps showing a null pointer exception.\n",
    "Agent: Which line is causing it?\n",
    "User: Line 42.\n",
    "Agent: That means you're accessing an object that wasn't initialized.\n",
    "User: How do I fix it?\n",
    "Agent: Add a null check or initialize the object before use.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n========== SUMMARY ==========\\n\")\n",
    "    print(summarize_text(text))\n",
    "    print(\"\\n=============================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc4e53-e76a-4c39-95eb-376a40573910",
   "metadata": {},
   "outputs": [],
   "source": [
    "the tortoise continued to plod along slowly and steadily, \n",
    "eventually crossing the finish line while the rabbit was asleep. \n",
    "the rabbit took the lead and, confident in his victory, decided to take a nap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e08c0-6b17-4b4b-97d6-6f6caacbff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Slow tortoise challenged rabbit to race. The rabbit took a nap,\n",
    "believing the turtle was too slow to catch up with him. It eventually \n",
    "crossed the finish line while the rabbit was asleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6dc37-6990-44f2-917b-7bd1089b9aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099203e-baef-44d3-b2e6-f8af1a13575b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ec1e7-ef64-4946-810d-423cf5672d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "your paragraphs should remind your reader that there is a recurrent relationship between your thesis and the information in each paragraph. a working thesis functions like a seed from which your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e8c62-7ac1-43c1-a9d2-dc08409cfc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f06bb-94fc-4d0f-b8ea-2c08dc286033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b61d3d-8a5f-4e64-971b-fd5c8148d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv(\"data/processed_test.csv\")\n",
    "\n",
    "df_test[\"generated_summary\"] = df_test[\"input_text\"].apply(summarize_text)\n",
    "\n",
    "df_test.to_csv(\"data/test_predictions_t5.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to data/test_predictions_t5.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4d742-3032-4de1-a9d7-649b4a4d0b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
